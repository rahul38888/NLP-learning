{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install tensorrt\n# !pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-28T01:14:12.825116Z","iopub.execute_input":"2024-09-28T01:14:12.826046Z","iopub.status.idle":"2024-09-28T01:14:12.846106Z","shell.execute_reply.started":"2024-09-28T01:14:12.825993Z","shell.execute_reply":"2024-09-28T01:14:12.845012Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline","metadata":{"execution":{"iopub.status.busy":"2024-09-28T01:14:14.232850Z","iopub.execute_input":"2024-09-28T01:14:14.233249Z","iopub.status.idle":"2024-09-28T01:14:33.823470Z","shell.execute_reply.started":"2024-09-28T01:14:14.233212Z","shell.execute_reply":"2024-09-28T01:14:33.822421Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"classifier = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\nclassifier([\"I am so excited for the mars sapce mission\", \"Things is about to get much worse\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-28T01:29:40.740600Z","iopub.execute_input":"2024-09-28T01:29:40.741050Z","iopub.status.idle":"2024-09-28T01:29:41.174670Z","shell.execute_reply.started":"2024-09-28T01:29:40.741011Z","shell.execute_reply":"2024-09-28T01:29:41.173602Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"[{'label': 'POSITIVE', 'score': 0.9987338185310364},\n {'label': 'NEGATIVE', 'score': 0.9997950196266174}]"},"metadata":{}}]},{"cell_type":"code","source":"generator = pipeline(\"text-generation\", model=\"distilgpt2\")\ngenerator(\"In 20 years, the world would be a place where you can\", max_length=50, num_return_sequences=4)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T01:29:49.998739Z","iopub.execute_input":"2024-09-28T01:29:49.999740Z","iopub.status.idle":"2024-09-28T01:29:52.205824Z","shell.execute_reply.started":"2024-09-28T01:29:49.999687Z","shell.execute_reply":"2024-09-28T01:29:52.204625Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[{'generated_text': \"In 20 years, the world would be a place where you can get a little stuff in someplace that's cheaper to do than to put it in a landfill. Of course the world would be different if you bought a house in a country like India\"},\n {'generated_text': 'In 20 years, the world would be a place where you can think of your own stories, work on your own stories, and come up with new ways to express yourself in a way that is unique in all aspects of life.\\n\\n\\nThe'},\n {'generated_text': 'In 20 years, the world would be a place where you can play.\\n\\n\\nWe\\u200ds an exciting, beautiful environment by creating an environment with the best of the best of the brightest.'},\n {'generated_text': 'In 20 years, the world would be a place where you can live and have friends.\\n\\n\\nA few years ago, the \"Great Fire and the Dragon\" (the English words derived from God the Great Fire/Dragon). \"The Great'}]"},"metadata":{}}]},{"cell_type":"code","source":"unmasker = pipeline(\"fill-mask\")\nunmasker(\"Once upon a time a <mask> lived in a house in a town\", top_k=4)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T01:43:18.521544Z","iopub.execute_input":"2024-09-28T01:43:18.522402Z","iopub.status.idle":"2024-09-28T01:43:19.391936Z","shell.execute_reply.started":"2024-09-28T01:43:18.522358Z","shell.execute_reply":"2024-09-28T01:43:19.390849Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to distilbert/distilroberta-base and revision ec58a5b (https://huggingface.co/distilbert/distilroberta-base).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nSome weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[{'score': 0.06463515758514404,\n  'token': 920,\n  'token_str': ' child',\n  'sequence': 'Once upon a time a child lived in a house in a town'},\n {'score': 0.04292440414428711,\n  'token': 2143,\n  'token_str': ' boy',\n  'sequence': 'Once upon a time a boy lived in a house in a town'},\n {'score': 0.03667924553155899,\n  'token': 1816,\n  'token_str': ' girl',\n  'sequence': 'Once upon a time a girl lived in a house in a town'},\n {'score': 0.0364205427467823,\n  'token': 313,\n  'token_str': ' man',\n  'sequence': 'Once upon a time a man lived in a house in a town'}]"},"metadata":{}}]},{"cell_type":"code","source":"ner = pipeline(\"ner\", grouped_entities=True)\nner(\"Rahul wants to work with Indian Space Research Organisation in India\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T01:48:41.253732Z","iopub.execute_input":"2024-09-28T01:48:41.254775Z","iopub.status.idle":"2024-09-28T01:48:42.024137Z","shell.execute_reply.started":"2024-09-28T01:48:41.254719Z","shell.execute_reply":"2024-09-28T01:48:42.023030Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nSome weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[{'entity_group': 'PER',\n  'score': 0.99239355,\n  'word': 'Rahul',\n  'start': 0,\n  'end': 5},\n {'entity_group': 'ORG',\n  'score': 0.99889565,\n  'word': 'Indian Space Research Organisation',\n  'start': 25,\n  'end': 59},\n {'entity_group': 'LOC',\n  'score': 0.99962676,\n  'word': 'India',\n  'start': 63,\n  'end': 68}]"},"metadata":{}}]},{"cell_type":"code","source":"qna = pipeline(\"question-answering\")\nprint(qna(question=\"What is Rahul's dream?\", context=\"Rahul wants to work with Indian Space Research Organisation in India\"))\nprint(qna(question=\"What space agency we are talking about?\", context=\"Rahul wants to work with Indian Space Research Organisation in India\"))","metadata":{"execution":{"iopub.status.busy":"2024-09-28T01:51:43.023114Z","iopub.execute_input":"2024-09-28T01:51:43.023571Z","iopub.status.idle":"2024-09-28T01:51:43.463129Z","shell.execute_reply.started":"2024-09-28T01:51:43.023528Z","shell.execute_reply":"2024-09-28T01:51:43.461941Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"name":"stdout","text":"{'score': 0.16895562410354614, 'start': 15, 'end': 59, 'answer': 'work with Indian Space Research Organisation'}\n{'score': 0.9684596061706543, 'start': 25, 'end': 59, 'answer': 'Indian Space Research Organisation'}\n","output_type":"stream"}]},{"cell_type":"code","source":"summarization=pipeline(\"summarization\")\nsummarization(\"\"\"\nThe ant and the grasshopper were good friends. In the summer, the ant works hard to fill his \nstorage with food. While the grasshopper was enjoying the fine weather and playing all day. \nWhen winter came, the ant was lying cozily in his home, surrounded by the food he stored during \nthe summer. While the grasshopper was in his home, hungry and freezing. He asked the ant for food, \nand the ant gave him some. But it wasn’t enough to last the entire winter. When he tried to ask \nthe ant again, the latter replied: “I’m sorry my friend but my food is just enough for my family \nto last until the end of winter. If I give you more, we too will starve. We had the entire summer \nto prepare for the winter but you chose to play instead.”\n\nMoral of the story:\nWinter, in this story, represents a time in our lives when food and resources are scarce. Summer \nis that time when everything is abundant. So, if you have a lot right now, save some of it for the winter.\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T01:56:34.537901Z","iopub.execute_input":"2024-09-28T01:56:34.538383Z","iopub.status.idle":"2024-09-28T01:56:45.412117Z","shell.execute_reply.started":"2024-09-28T01:56:34.538344Z","shell.execute_reply":"2024-09-28T01:56:45.411061Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"[{'summary_text': \" In the summer, the ant works hard to fill his storage with food . While the grasshopper was enjoying the fine weather and playing all day, he was hungry and freezing . He asked the ant for food, but it wasn't enough to last the entire winter . The ant replied: “I'm sorry my friend but my food is just enough for my family  to last until the end of winter. If I give you more, we too will starve.”\"}]"},"metadata":{}}]},{"cell_type":"code","source":"# translate = pipeline(\"translation\", model=\"alirezamsh/small100\")\n# translate(\"The ant and the grasshopper were good friends\", tgt_lang=\"hi\", src_lang=\"en\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:19:57.268026Z","iopub.execute_input":"2024-09-28T02:19:57.268905Z","iopub.status.idle":"2024-09-28T02:19:57.273727Z","shell.execute_reply.started":"2024-09-28T02:19:57.268860Z","shell.execute_reply":"2024-09-28T02:19:57.272350Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}